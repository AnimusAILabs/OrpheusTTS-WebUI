<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Streaming Audio Playback (Local Development)</title>
  <style>
    body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
    .status { margin: 20px 0; padding: 10px; border-radius: 4px; }
    .controls { margin: 20px 0; }
    button { padding: 8px 16px; margin-right: 10px; }
    #promptInput { width: 100%; margin: 10px 0; }
    #progress { 
      width: 100%; 
      height: 20px; 
      background: #f0f0f0;
      border-radius: 4px;
      margin: 20px 0;
    }
    #progressBar {
      width: 0%;
      height: 100%;
      background: #4CAF50;
      border-radius: 4px;
      transition: width 0.3s ease;
    }
    #debug { 
      margin-top: 20px;
      padding: 10px;
      background: #f5f5f5;
      border-radius: 4px;
      font-family: monospace;
      white-space: pre-wrap;
    }
  </style>
</head>
<body>
  <h1>Streaming Audio Playback</h1>
  <form id="promptForm">
    <label for="promptInput">Enter Prompt:</label><br>
    <textarea id="promptInput" rows="4" cols="50" placeholder="Type your prompt here" required></textarea><br>
    <button type="submit">Generate Audio</button>
  </form>
  <div class="controls">
    <button id="pauseButton" disabled>Pause</button>
    <button id="resumeButton" disabled>Resume</button>
  </div>
  <div id="status" class="status"></div>
  <div id="progress">
    <div id="progressBar"></div>
  </div>
  <div id="debug"></div>

  <script>
    // Use the RunPod WebSocket URL directly
    const wsUrl = 'wss://umukid49349ire-8080.proxy.runpod.net/ws';
    const ws = new WebSocket(wsUrl);
    let audioContext = null;
    let audioQueue = [];
    let isPlaying = false;
    let startTime = 0;
    let totalDuration = 0;
    let processedDuration = 0;
    let debugLog = [];
    let bufferSize = 0.4; // Buffer size in seconds
    let nextPlayTime = 0;
    let currentContextId = null;
    let audioBuffers = new Map(); // Store audio buffers by context ID

    function showStatus(message) {
      document.getElementById('status').textContent = message;
      debugLog.push(`[${new Date().toISOString()}] ${message}`);
      updateDebug();
    }

    function updateProgress(progress) {
      document.getElementById('progressBar').style.width = `${progress}%`;
    }

    function updateDebug() {
      document.getElementById('debug').textContent = debugLog.join('\n');
    }

    // Initialize audio context
    function initAudio() {
      if (!audioContext) {
        audioContext = new (window.AudioContext || window.webkitAudioContext)({sampleRate: 24000});
        debugLog.push(`[${new Date().toISOString()}] AudioContext initialized with sample rate: ${audioContext.sampleRate}`);
        updateDebug();
      }
    }

    // Create a new audio context
    function createAudioContext(contextId) {
      audioBuffers.set(contextId, []);
      currentContextId = contextId;
      debugLog.push(`[${new Date().toISOString()}] Created new audio context: ${contextId}`);
      updateDebug();
    }

    // Append audio to context
    function appendToAudioContext(contextId, audioData) {
      if (!audioBuffers.has(contextId)) {
        createAudioContext(contextId);
      }
      audioBuffers.get(contextId).push(audioData);
      debugLog.push(`[${new Date().toISOString()}] Appended ${audioData.length} samples to context ${contextId}`);
      updateDebug();
    }

    // Process audio chunks
    function processAudioChunk(chunk, contextId) {
      if (!audioContext || !isPlaying) return;

      try {
        const buffer = audioContext.createBuffer(1, chunk.length, 24000);
        buffer.copyToChannel(chunk, 0);
        
        const source = audioContext.createBufferSource();
        source.buffer = buffer;
        source.connect(audioContext.destination);
        
        // Schedule the chunk to play at the next available time
        const currentTime = audioContext.currentTime;
        if (nextPlayTime < currentTime) {
          nextPlayTime = currentTime;
        }
        
        source.start(nextPlayTime);
        nextPlayTime += chunk.length / 24000;
        
        processedDuration += chunk.length / 24000;
        updateProgress((processedDuration / totalDuration) * 100);
        
        debugLog.push(`[${new Date().toISOString()}] Scheduled chunk of size ${chunk.length} samples at ${nextPlayTime.toFixed(3)}s`);
        updateDebug();
      } catch (error) {
        debugLog.push(`[${new Date().toISOString()}] Error processing chunk: ${error.message}`);
        updateDebug();
        console.error('Error processing audio chunk:', error);
      }
    }

    // Initialize controls
    const pauseButton = document.getElementById('pauseButton');
    const resumeButton = document.getElementById('resumeButton');
    
    pauseButton.onclick = () => {
      if (audioContext && isPlaying) {
        audioContext.suspend();
        isPlaying = false;
        pauseButton.disabled = true;
        resumeButton.disabled = false;
        showStatus('Playback paused');
      }
    };
    
    resumeButton.onclick = () => {
      if (audioContext && !isPlaying) {
        audioContext.resume();
        isPlaying = true;
        pauseButton.disabled = false;
        resumeButton.disabled = true;
        showStatus('Playing audio...');
        
        // Process any queued chunks
        while (audioQueue.length > 0) {
          processAudioChunk(audioQueue.shift(), currentContextId);
        }
      }
    };

    // WebSocket event handlers
    ws.onopen = () => {
      console.log('Connected to RunPod server');
      showStatus('Connected to RunPod server. Click Generate Audio to begin.');
    };

    ws.onclose = () => {
      console.log('Disconnected from RunPod server');
      showStatus('Disconnected from RunPod server');
    };

    ws.onerror = (error) => {
      console.error('WebSocket error:', error);
      showStatus('WebSocket error occurred');
    };

    ws.onmessage = async (event) => {
      const data = JSON.parse(event.data);
      
      if (data.type === 'audio_chunk') {
        const chunk = new Uint8Array(hexToArrayBuffer(data.chunk));
        const contextId = data.context_id || 'default';
        debugLog.push(`[${new Date().toISOString()}] Received chunk of size ${chunk.length} bytes for context ${contextId}`);
        updateDebug();
        
        if (!audioContext) {
          initAudio();
          isPlaying = true;
          pauseButton.disabled = false;
          resumeButton.disabled = true;
          showStatus('Playing audio...');
          nextPlayTime = audioContext.currentTime + bufferSize; // Add initial buffer
        }

        try {
          // Convert 16-bit PCM to float32
          const floatData = new Float32Array(chunk.length / 2);
          const view = new DataView(chunk.buffer);
          for (let i = 0; i < floatData.length; i++) {
            floatData[i] = view.getInt16(i * 2, true) / 32768.0;
          }
          
          if (isPlaying) {
            processAudioChunk(floatData, contextId);
          } else {
            audioQueue.push(floatData);
          }
        } catch (error) {
          debugLog.push(`[${new Date().toISOString()}] Error converting chunk: ${error.message}`);
          updateDebug();
          console.error('Error processing audio chunk:', error);
        }
        
      } else if (data.type === 'generation_complete') {
        console.log('Audio generation completed');
        showStatus('Audio streaming completed');
        totalDuration = processedDuration;
        debugLog.push(`[${new Date().toISOString()}] Total duration: ${totalDuration.toFixed(2)} seconds`);
        updateDebug();
      }
    };

    // Convert hex string to ArrayBuffer
    function hexToArrayBuffer(hexString) {
      const bytes = new Uint8Array(hexString.length / 2);
      for (let i = 0; i < hexString.length; i += 2) {
        bytes[i / 2] = parseInt(hexString.substr(i, 2), 16);
      }
      return bytes.buffer;
    }

    // Form submission handler
    document.getElementById("promptForm").addEventListener("submit", async function(event) {
      event.preventDefault();
      
      // Reset state
      if (audioContext) {
        audioContext.suspend();
      }
      audioQueue = [];
      processedDuration = 0;
      totalDuration = 0;
      nextPlayTime = 0;
      audioBuffers.clear();
      currentContextId = null;
      updateProgress(0);
      debugLog = [];
      updateDebug();
      
      // Disable controls initially
      pauseButton.disabled = true;
      resumeButton.disabled = true;
      
      const prompt = document.getElementById("promptInput").value;
      showStatus('Generating audio...');
      
      // Request new audio generation
      ws.send(JSON.stringify({ prompt: prompt }));
    });
  </script>
</body>
</html> 