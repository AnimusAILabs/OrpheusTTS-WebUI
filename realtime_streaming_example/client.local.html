<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Streaming Audio Playback (Local Development)</title>
  <style>
    body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
    .status { margin: 20px 0; padding: 10px; border-radius: 4px; }
    .controls { margin: 20px 0; }
    button { padding: 8px 16px; margin-right: 10px; }
    #promptInput { width: 100%; margin: 10px 0; }
    #progress { 
      width: 100%; 
      height: 20px; 
      background: #f0f0f0;
      border-radius: 4px;
      margin: 20px 0;
    }
    #progressBar {
      width: 0%;
      height: 100%;
      background: #4CAF50;
      border-radius: 4px;
      transition: width 0.3s ease;
    }
    #debug { 
      margin-top: 20px;
      padding: 10px;
      background: #f5f5f5;
      border-radius: 4px;
      font-family: monospace;
      white-space: pre-wrap;
      max-height: 300px;
      overflow-y: auto;
    }
  </style>
</head>
<body>
  <h1>Streaming Audio Playback</h1>
  <form id="promptForm">
    <label for="promptInput">Enter Prompt:</label><br>
    <textarea id="promptInput" rows="4" cols="50" placeholder="Type your prompt here" required></textarea><br>
    <button type="submit">Generate Audio</button>
  </form>
  <div class="controls">
    <button id="pauseButton" disabled>Pause</button>
    <button id="resumeButton" disabled>Resume</button>
  </div>
  <div id="status" class="status"></div>
  <div id="progress">
    <div id="progressBar"></div>
  </div>
  <div id="debug"></div>

  <script>
    // Use the RunPod WebSocket URL directly
    const wsUrl = 'wss://umukid49349ire-8080.proxy.runpod.net/ws';
    let ws = null;
    let audioContext = null;
    let audioQueue = [];
    let isPlaying = false;
    let startTime = 0;
    let totalDuration = 0;
    let processedDuration = 0;
    let debugLog = [];
    let bufferSize = 1.0; // Buffer size in seconds
    let nextPlayTime = 0;
    let currentContextId = null;
    let audioBuffers = new Map(); // Store audio buffers by context ID
    let isBuffering = true;
    let minBufferSize = 0.5; // Minimum buffer size in seconds before starting playback
    let isWebSocketReady = false;
    let pendingPrompt = null;
    let audioWorkletNode = null;
    let audioWorkletProcessor = null;

    function showStatus(message) {
      document.getElementById('status').textContent = message;
      debugLog.push(`[${new Date().toISOString()}] ${message}`);
      updateDebug();
    }

    function updateProgress(progress) {
      document.getElementById('progressBar').style.width = `${progress}%`;
    }

    function updateDebug() {
      const debugElement = document.getElementById('debug');
      debugElement.textContent = debugLog.join('\n');
      debugElement.scrollTop = debugElement.scrollHeight;
    }

    // Initialize audio context
    async function initAudio() {
      if (!audioContext) {
        audioContext = new (window.AudioContext || window.webkitAudioContext)({sampleRate: 24000});
        debugLog.push(`[${new Date().toISOString()}] AudioContext initialized with sample rate: ${audioContext.sampleRate}`);
        updateDebug();
      }
    }

    // Create a new audio context
    function createAudioContext(contextId) {
      audioBuffers.set(contextId, []);
      currentContextId = contextId;
      isBuffering = true;
      debugLog.push(`[${new Date().toISOString()}] Created new audio context: ${contextId}`);
      updateDebug();
    }

    // Append audio to context
    function appendToAudioContext(contextId, audioData) {
      if (!audioBuffers.has(contextId)) {
        createAudioContext(contextId);
      }
      audioBuffers.get(contextId).push(audioData);
      debugLog.push(`[${new Date().toISOString()}] Appended ${audioData.length} samples to context ${contextId}`);
      updateDebug();
    }

    // Process audio chunks
    function processAudioChunk(chunk, contextId) {
      if (!audioContext || !isPlaying) return;

      try {
        const buffer = audioContext.createBuffer(1, chunk.length, 24000);
        buffer.copyToChannel(chunk, 0);
        
        const source = audioContext.createBufferSource();
        source.buffer = buffer;
        source.connect(audioContext.destination);
        
        // Schedule the chunk to play at the next available time
        const currentTime = audioContext.currentTime;
        if (nextPlayTime < currentTime) {
          nextPlayTime = currentTime;
        }
        
        source.start(nextPlayTime);
        nextPlayTime += chunk.length / 24000;
        
        processedDuration += chunk.length / 24000;
        updateProgress((processedDuration / totalDuration) * 100);
        
        debugLog.push(`[${new Date().toISOString()}] Scheduled chunk of size ${chunk.length} samples at ${nextPlayTime.toFixed(3)}s`);
        updateDebug();
      } catch (error) {
        debugLog.push(`[${new Date().toISOString()}] Error processing chunk: ${error.message}`);
        updateDebug();
        console.error('Error processing audio chunk:', error);
      }
    }

    // Initialize WebSocket connection
    function initWebSocket() {
      return new Promise((resolve, reject) => {
        if (ws) {
          ws.close();
        }
        
        ws = new WebSocket(wsUrl);
        isWebSocketReady = false;
        
        ws.onopen = () => {
          console.log('Connected to RunPod server');
          showStatus('Connected to RunPod server. Click Generate Audio to begin.');
          isWebSocketReady = true;
          resolve();
        };

        ws.onclose = () => {
          console.log('Disconnected from RunPod server');
          showStatus('Disconnected from RunPod server');
          isWebSocketReady = false;
        };

        ws.onerror = (error) => {
          console.error('WebSocket error:', error);
          showStatus('WebSocket error occurred');
          isWebSocketReady = false;
          reject(error);
        };

        ws.onmessage = async (event) => {
          const data = JSON.parse(event.data);
          
          if (data.type === 'start') {
            currentContextId = data.context_id;
            createAudioContext(currentContextId);
            showStatus('Starting audio generation...');
          }
          
          if (data.type === 'audio_chunk') {
            const chunk = new Uint8Array(hexToArrayBuffer(data.chunk));
            const contextId = data.context_id || 'default';
            debugLog.push(`[${new Date().toISOString()}] Received chunk of size ${chunk.length} bytes for context ${contextId}`);
            updateDebug();
            
            if (!audioContext) {
              await initAudio();
              isPlaying = true;
              pauseButton.disabled = false;
              resumeButton.disabled = true;
              showStatus('Playing audio...');
              nextPlayTime = audioContext.currentTime + bufferSize; // Add initial buffer
            }

            try {
              // Convert 16-bit PCM to float32
              const floatData = new Float32Array(chunk.length / 2);
              const view = new DataView(chunk.buffer);
              for (let i = 0; i < floatData.length; i++) {
                floatData[i] = view.getInt16(i * 2, true) / 32768.0;
              }
              
              appendToAudioContext(contextId, floatData);
              
              // Check if we have enough buffer to start playing
              if (isBuffering) {
                const totalBufferedDuration = audioBuffers.get(contextId).reduce((acc, chunk) => acc + chunk.length / 24000, 0);
                if (totalBufferedDuration >= minBufferSize) {
                  isBuffering = false;
                  showStatus('Buffer filled, starting playback...');
                  // Process all buffered chunks
                  const buffers = audioBuffers.get(contextId);
                  audioBuffers.set(contextId, []);
                  buffers.forEach(chunk => processAudioChunk(chunk, contextId));
                }
              } else if (isPlaying) {
                processAudioChunk(floatData, contextId);
              }
            } catch (error) {
              debugLog.push(`[${new Date().toISOString()}] Error converting chunk: ${error.message}`);
              updateDebug();
              console.error('Error processing audio chunk:', error);
            }
          }
          
          if (data.type === 'generation_complete') {
            console.log('Audio generation completed');
            showStatus('Audio streaming completed');
            totalDuration = processedDuration;
            debugLog.push(`[${new Date().toISOString()}] Total duration: ${totalDuration.toFixed(2)} seconds`);
            updateDebug();
          }
        };
      });
    }

    // Initialize controls
    const pauseButton = document.getElementById('pauseButton');
    const resumeButton = document.getElementById('resumeButton');
    
    pauseButton.onclick = () => {
      if (audioContext && isPlaying) {
        audioContext.suspend();
        isPlaying = false;
        pauseButton.disabled = true;
        resumeButton.disabled = false;
        showStatus('Playback paused');
      }
    };
    
    resumeButton.onclick = () => {
      if (audioContext && !isPlaying) {
        audioContext.resume();
        isPlaying = true;
        pauseButton.disabled = false;
        resumeButton.disabled = true;
        showStatus('Playing audio...');
        
        // Process any queued chunks
        while (audioQueue.length > 0) {
          processAudioChunk(audioQueue.shift(), currentContextId);
        }
      }
    };

    // Convert hex string to ArrayBuffer
    function hexToArrayBuffer(hexString) {
      const bytes = new Uint8Array(hexString.length / 2);
      for (let i = 0; i < hexString.length; i += 2) {
        bytes[i / 2] = parseInt(hexString.substr(i, 2), 16);
      }
      return bytes.buffer;
    }

    // Clean up audio resources
    function cleanupAudio() {
      if (audioContext) {
        audioContext.suspend();
        audioContext.close();
        audioContext = null;
      }
      audioQueue = [];
      processedDuration = 0;
      totalDuration = 0;
      nextPlayTime = 0;
      audioBuffers.clear();
      currentContextId = null;
      isPlaying = false;
      isBuffering = true;
      updateProgress(0);
      debugLog = [];
      updateDebug();
      pauseButton.disabled = true;
      resumeButton.disabled = true;
    }

    // Form submission handler
    document.getElementById("promptForm").addEventListener("submit", async function(event) {
      event.preventDefault();
      
      // Clean up previous audio state
      cleanupAudio();
      
      try {
        // Initialize new WebSocket connection and wait for it to be ready
        await initWebSocket();
        
        const prompt = document.getElementById("promptInput").value;
        showStatus('Generating audio...');
        
        // Request new audio generation
        ws.send(JSON.stringify({ prompt: prompt }));
      } catch (error) {
        console.error('Error initializing WebSocket:', error);
        showStatus('Error connecting to server. Please try again.');
      }
    });

    // Initialize WebSocket on page load
    initWebSocket().catch(error => {
      console.error('Error initializing WebSocket:', error);
      showStatus('Error connecting to server. Please refresh the page.');
    });
  </script>
</body>
</html> 